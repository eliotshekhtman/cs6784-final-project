{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import dual_annealing\n",
    "from utils import GradientBandit, GradientBanditNoStrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantReward:\n",
    "    def __init__(self,arms):\n",
    "        reward = np.random.rand(arms)\n",
    "        reward /= np.sum(reward)\n",
    "        self.reward = reward\n",
    "\n",
    "    def get_reward(self,context):\n",
    "        return self.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategicAgents:\n",
    "    def __init__(self, dim, delta_radius, reward):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.reward = reward\n",
    "        self.dim = dim\n",
    "\n",
    "    def generate_context(self, policy, priv):\n",
    "        reward = self.reward.get_reward(priv)\n",
    "        bounds = [(-self.delta_radius, self.delta_radius)] * (self.dim)\n",
    "        def objective(delta):\n",
    "            if np.linalg.norm(delta) > self.delta_radius:\n",
    "                return np.inf\n",
    "            #x_prime = np.append((priv+delta),[1])\n",
    "            x_prime = priv+delta\n",
    "            return -reward[np.argmax(policy@x_prime)]\n",
    "\n",
    "        opt_delt = dual_annealing(objective, bounds)\n",
    "        x_prime = priv + opt_delt.x\n",
    "\n",
    "        return x_prime, reward[np.argmax(policy@x_prime)]\n",
    "        #return x_prime, reward[np.argmax(policy@np.append(x_prime,[1]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImperfectStrategicAgents:\n",
    "    def __init__(self, dim, delta_radius, reward, policy_noise):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.reward = reward\n",
    "        self.dim = dim\n",
    "        self.policy_noise = policy_noise\n",
    "\n",
    "    def generate_context(self, policy, priv):\n",
    "        imp_policy = policy + self.policy_noise*(np.random.random(policy.shape)-0.5)\n",
    "        reward = self.reward.get_reward(priv)\n",
    "        bounds = [(-self.delta_radius, self.delta_radius)] * (self.dim)\n",
    "        def objective(delta):\n",
    "            if np.linalg.norm(delta) > self.delta_radius:\n",
    "                return np.inf\n",
    "            #x_prime = np.append((priv+delta),[1])\n",
    "            x_prime = priv+delta\n",
    "            return -reward[np.argmax(imp_policy@x_prime)]\n",
    "\n",
    "        opt_delt = dual_annealing(objective, bounds)\n",
    "        x_prime = priv + opt_delt.x\n",
    "\n",
    "        return x_prime, reward[np.argmax(policy@x_prime)]\n",
    "        #return x_prime, reward[np.argmax(policy@np.append(x_prime,[1]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStrategicAgents:\n",
    "    def __init__(self,  dim, delta_radius, reward):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.reward = reward\n",
    "        self.dim = dim\n",
    "\n",
    "    def generate_context(self, policy, priv):\n",
    "        reward = self.reward.get_reward(priv)\n",
    "\n",
    "        return priv, reward[np.argmax(policy@priv)]\n",
    "        #return priv, reward[np.argmax(policy@np.append(priv,[1]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = 100\n",
    "# ARMS = 4\n",
    "# CONTEXT_DIM = 5\n",
    "# delta_radius = 0.5\n",
    "\n",
    "# private_types = np.random.rand(T,CONTEXT_DIM)\n",
    "# rewards = ConstantReward(ARMS)\n",
    "# strat_agents = StrategicAgents(private_types,CONTEXT_DIM,delta_radius,rewards)\n",
    "# nostrat_agents = NonStrategicAgents(private_types,CONTEXT_DIM,delta_radius,rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = np.random.rand(CONTEXT_DIM,ARMS) - 0.5\n",
    "# print(\"policy: {} \".format(policy))\n",
    "# print(\"reward: {} \".format(rewards.get_reward(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check to make sure the strategic agents are performing better than non-strategic agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_strat_reward = 0\n",
    "# total_nostrat_reward = 0\n",
    "# for i in range(T):\n",
    "#     xp, strat_reward = strat_agents.generate_context(policy)\n",
    "#     x, nostrat_reward = nostrat_agents.generate_context(policy)\n",
    "#     total_strat_reward += strat_reward\n",
    "#     total_nostrat_reward += nostrat_reward\n",
    "\n",
    "# print(\"strategic reward: {}, non-strategic reward: {}\".format(total_strat_reward,total_nostrat_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelReward:\n",
    "    def __init__(self,arms,dim,noise=0.01):\n",
    "        self.theta = np.random.rand(arms,dim)\n",
    "        self.noise_scale = noise\n",
    "\n",
    "    def get_reward(self,action,context):\n",
    "        true_reward = action @ self.theta @ context\n",
    "        return true_reward + (np.random.rand(1)-0.5) * 2*self.noise_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyAwareGradientModel:\n",
    "    def __init__(self,T,est_reward,lr,dim,delta_radius,arms,reward_func):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.t = 0\n",
    "        self.T = T\n",
    "        self.est_agent_reward = est_reward\n",
    "        self.dim = dim\n",
    "        self.model = GradientBandit(arms,dim,bias=False)\n",
    "        self.reward_function = reward_func\n",
    "        self.opt = torch.optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "\n",
    "    def get_policy(self):\n",
    "        return self.model.get_hyperplanes()\n",
    "\n",
    "    def observe_reward(self, context):\n",
    "        self.opt.zero_grad()\n",
    "        context = torch.tensor(context).reshape(1,-1).float()\n",
    "        est_agent_rew = torch.tensor(self.est_agent_reward.get_reward(context)).float()\n",
    "        y_hat = self.model.forward(context,est_agent_rew)\n",
    "        # print(y_hat)\n",
    "        reward = self.reward_function.get_reward(y_hat,context)\n",
    "        loss = self.criterion(1/reward,torch.zeros(1))\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyUnawareGradientModel:\n",
    "    def __init__(self,T,est_reward,lr,dim,delta_radius,arms,reward_func):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.t = 0\n",
    "        self.T = T\n",
    "        self.est_agent_reward = est_reward\n",
    "        self.dim = dim\n",
    "        self.model = GradientBanditNoStrat(arms,dim,bias=False)\n",
    "        self.reward_function = reward_func\n",
    "        self.opt = torch.optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "\n",
    "    def get_policy(self):\n",
    "        return self.model.get_hyperplanes()\n",
    "\n",
    "    def different_(self, x_true, x_observed):\n",
    "        est_agent_rew = self.est_agent_reward.get_reward(None)\n",
    "        y_true = self.model.forward(torch.tensor(x_true).float(),est_agent_rew)\n",
    "        y_obv = self.model.forward(torch.tensor(x_observed).float(),est_agent_rew)\n",
    "        if torch.argmax(y_obv) != torch.argmax(y_true):\n",
    "            better = est_agent_rew[torch.argmax(y_obv)] > est_agent_rew[torch.argmax(y_true)]\n",
    "            return True, better\n",
    "        else:\n",
    "            return False, False\n",
    "\n",
    "    def observe_reward(self, context):\n",
    "        self.opt.zero_grad()\n",
    "        context = torch.tensor(context).reshape(1,-1).float()\n",
    "        est_agent_rew = torch.tensor(self.est_agent_reward.get_reward(context)).float()\n",
    "        y_hat = self.model.forward(context,est_agent_rew)\n",
    "        reward = self.reward_function.get_reward(y_hat,context)\n",
    "        loss = self.criterion(1/reward,torch.zeros(1))\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyUnawareModel:\n",
    "    def __init__(self,T,est_reward,lr,dim,delta_radius,arms,reward_func):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.t = 0\n",
    "        self.T = T\n",
    "        self.est_agent_reward = est_reward\n",
    "        self.dim = dim\n",
    "        self.policy= np.random.rand(arms,dim)\n",
    "        self.reward_function = reward_func\n",
    "        self.committed = False\n",
    "        self.arms = arms\n",
    "        rewards = [ [] for i in range(self.arms)]\n",
    "        contexts = [ [] for i in range(self.arms)]\n",
    "        for arm in range(arms):\n",
    "            rewards[arm] = []\n",
    "            contexts[arm] = []\n",
    "        \n",
    "        self.reward_data = rewards\n",
    "        self.context_data = contexts\n",
    "\n",
    "    def refresh_policy(self):\n",
    "        self.policy= np.random.rand(self.arms,self.dim)        \n",
    "\n",
    "    def get_policy(self):\n",
    "        return self.policy\n",
    "\n",
    "    def committ(self):\n",
    "        theta = np.zeros((self.arms,self.dim))\n",
    "        for arm in range(self.arms):\n",
    "            xs = np.asarray(self.context_data[arm])\n",
    "            ys = np.asarray(self.reward_data[arm])\n",
    "            \n",
    "            est_theta = np.linalg.lstsq(xs,ys,rcond=None)[0]\n",
    "            theta[arm,:] = est_theta.flatten()\n",
    "\n",
    "        self.policy = theta\n",
    "\n",
    "    def different_(self, x_true, x_observed):\n",
    "        est_agent_rew = self.est_agent_reward.get_reward(None)\n",
    "        y_true = self.policy@x_true\n",
    "        y_obv = self.policy@x_observed\n",
    "        if np.argmax(y_obv) != np.argmax(y_true):\n",
    "            better = est_agent_rew[np.argmax(y_obv)] > est_agent_rew[np.argmax(y_true)]\n",
    "            return True, better\n",
    "        else:\n",
    "            return False, False\n",
    "\n",
    "    def get_action(self,context):\n",
    "        expected_rewards = self.policy@context\n",
    "        action = np.argmax(expected_rewards)\n",
    "        return action\n",
    "\n",
    "    def observe_reward(self, action, context):\n",
    "        est_agent_rew = self.est_agent_reward.get_reward(context)\n",
    "\n",
    "        action_vector = np.zeros(self.arms)\n",
    "        action_vector[action] += 1\n",
    "        reward = self.reward_function.get_reward(action_vector,context)\n",
    "\n",
    "        self.reward_data[action] += [reward]\n",
    "        self.context_data[action] += [context]\n",
    "\n",
    "        return reward, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1200\n",
    "ARMS = 4\n",
    "CONTEXT_DIM = 5\n",
    "DELTA = 0.25\n",
    "lr = 0.1\n",
    "model_reward_noise = 0.05\n",
    "imperfect_agent_policy_noise = 0.05\n",
    "\n",
    "private_types = np.random.rand(T,CONTEXT_DIM)\n",
    "agent_rewards = ConstantReward(ARMS)\n",
    "model_reward_func = ModelReward(ARMS,CONTEXT_DIM,model_reward_noise)\n",
    "strat_agents = StrategicAgents(CONTEXT_DIM,DELTA,agent_rewards)\n",
    "imperfect_agents = ImperfectStrategicAgents(CONTEXT_DIM,DELTA,agent_rewards,imperfect_agent_policy_noise)\n",
    "nostrat_agents = NonStrategicAgents(CONTEXT_DIM,DELTA,agent_rewards)\n",
    "model = StrategyUnawareModel(T,agent_rewards,lr,CONTEXT_DIM,DELTA,ARMS,model_reward_func)\n",
    "\n",
    "\n",
    "strat_agent_errors = 0\n",
    "impf_agent_error = 0\n",
    "regret = [0]\n",
    "for i in range(T):\n",
    "    if i % 3 == 0:\n",
    "        x,agent_reward = nostrat_agents.generate_context(model.get_policy(), private_types[i])\n",
    "    elif i % 3 == 1:\n",
    "        x, agent_reward = imperfect_agents.generate_context(model.get_policy(), private_types[i])\n",
    "    else:\n",
    "        x,agent_reward = strat_agents.generate_context(model.get_policy(), private_types[i])\n",
    "    \n",
    "    action = model.get_action(x)\n",
    "    reward = model.observe_reward(action,x)\n",
    "    different, better = model.different_(x,private_types[i])\n",
    "    true_act = np.argmax(model_reward_func.theta @ private_types[i])\n",
    "    regret.append(regret[-1] + (true_act != action))\n",
    "    if i < T//2:\n",
    "        model.refresh_policy()\n",
    "    elif i == T//2:\n",
    "        model.committ()\n",
    "\n",
    "    if different:\n",
    "        if i % 3 == 1:\n",
    "            impf_agent_error += 1\n",
    "        else:\n",
    "            strat_agent_errors += 1\n",
    "\n",
    "print(\"strat errors: {} | impf errors: {}\".format(strat_agent_errors,impf_agent_error))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = range(T)\n",
    "plt.scatter(x, regret[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00764596 -0.01625134 -0.00687294 -0.01193293  0.05755617]\n",
      " [-0.02455891 -0.00220384 -0.00910528 -0.00386984  0.00795818]\n",
      " [ 0.04274479 -0.10335167  0.04604875  0.03231198  0.04248598]\n",
      " [ 0.09833547 -0.06452297 -0.09266993  0.06593723  0.00417664]]\n"
     ]
    }
   ],
   "source": [
    "print(model.get_policy() - model_reward_func.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "strat: 0.3505486060025757 | impf: 0.33543223262133254 | no strat: 0.3435960223000631\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x = range(T//3)\n",
    "# fig, axs = plt.subplots(3, sharex=True, sharey=False)\n",
    "# print(len(x))\n",
    "# print(len([x.item() for x in model_rewards_strat]))\n",
    "# axs[0].scatter(x, [x.item() for x in model_rewards_strat])\n",
    "# axs[1].scatter(x, [x.item() for x in model_rewards_imp])\n",
    "# axs[2].scatter(x, [x.item() for x in model_rewards_ns])\n",
    "# print(\"strat: {} | impf: {} | no strat: {}\".format(sum([3*x.item() for x in model_rewards_strat])/T, sum([3*x.item() for x in model_rewards_imp])/T,sum([3*x.item() for x in model_rewards_ns])/T,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12e07b149c03f77d4bac3a4791c0a52eb40c19a4b80558fffc7bbb9c81e36b37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
