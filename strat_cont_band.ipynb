{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import dual_annealing\n",
    "from utils import GradientBandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantReward:\n",
    "    def __init__(self,arms):\n",
    "        reward = np.random.rand(arms)\n",
    "        reward /= np.sum(reward)\n",
    "        self.reward = reward\n",
    "\n",
    "    def get_reward(self,context):\n",
    "        return self.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategicAgents:\n",
    "    def __init__(self, private_types, dim, delta_radius, reward):\n",
    "        self.private_types = private_types\n",
    "        self.delta_radius = delta_radius\n",
    "        self.t = 0\n",
    "        self.reward = reward\n",
    "        self.dim = dim\n",
    "\n",
    "    def generate_context(self, policy):\n",
    "        priv = self.private_types[self.t]\n",
    "        reward = self.reward.get_reward(priv)\n",
    "        bounds = [(-self.delta_radius, self.delta_radius)] * (self.dim)\n",
    "        def objective(delta):\n",
    "            if np.linalg.norm(delta) > self.delta_radius:\n",
    "                return np.inf\n",
    "            x_prime = np.append((priv+delta),[1])\n",
    "            return -reward[np.argmax(policy@x_prime)]\n",
    "\n",
    "        opt_delt = dual_annealing(objective, bounds)\n",
    "        x_prime = priv + opt_delt.x\n",
    "        self.t += 1\n",
    "        return x_prime, reward[np.argmax(policy@np.append(x_prime,[1]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStrategicAgents:\n",
    "    def __init__(self, private_types, dim, delta_radius, reward):\n",
    "        self.private_types = private_types\n",
    "        self.delta_radius = delta_radius\n",
    "        self.t = 0\n",
    "        self.reward = reward\n",
    "        self.dim = dim\n",
    "\n",
    "    def generate_context(self, policy):\n",
    "        priv = self.private_types[self.t],[1]\n",
    "        reward = self.reward.get_reward(priv)\n",
    "\n",
    "        self.t += 1\n",
    "        return priv, reward[np.argmax(policy@np.append(priv,[1]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "ARMS = 4\n",
    "CONTEXT_DIM = 5\n",
    "delta_radius = 0.5\n",
    "\n",
    "private_types = np.random.rand(T,CONTEXT_DIM)\n",
    "rewards = ConstantReward(ARMS)\n",
    "strat_agents = StrategicAgents(private_types,CONTEXT_DIM,delta_radius,rewards)\n",
    "nostrat_agents = NonStrategicAgents(private_types,CONTEXT_DIM,delta_radius,rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy: [[ 0.31827152 -0.06689342  0.39410267  0.40178453]\n",
      " [-0.15655195 -0.23157348 -0.12652979 -0.29663049]\n",
      " [ 0.10962736 -0.48593396  0.10674391  0.21765811]\n",
      " [ 0.11098109 -0.25367018 -0.37081219 -0.33286856]\n",
      " [-0.28542119  0.35176982 -0.43220067  0.04631573]] \n",
      "reward: [0.2942194  0.02555277 0.36679221 0.31343562] \n"
     ]
    }
   ],
   "source": [
    "policy = np.random.rand(CONTEXT_DIM,ARMS) - 0.5\n",
    "print(\"policy: {} \".format(policy))\n",
    "print(\"reward: {} \".format(rewards.get_reward(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check to make sure the strategic agents are performing better than non-strategic agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DeanA\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:497: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x) - f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategic reward: 32.27858860198842, non-strategic reward: 29.226927653796203\n"
     ]
    }
   ],
   "source": [
    "total_strat_reward = 0\n",
    "total_nostrat_reward = 0\n",
    "for i in range(T):\n",
    "    xp, strat_reward = strat_agents.generate_context(policy)\n",
    "    x, nostrat_reward = nostrat_agents.generate_context(policy)\n",
    "    total_strat_reward += strat_reward\n",
    "    total_nostrat_reward += nostrat_reward\n",
    "\n",
    "print(\"strategic reward: {}, non-strategic reward: {}\".format(total_strat_reward,total_nostrat_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelReward:\n",
    "    def __init__(self,arms,dim,noise=0.1):\n",
    "        self.theta = torch.rand(arms,dim)\n",
    "        self.noise_scale = noise\n",
    "\n",
    "    def get_reward(self,action,context):\n",
    "        true_reward = action @ self.theta @ torch.t(context)\n",
    "        return true_reward + (torch.rand(1)-0.5) * 2*self.noise_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyAwareGradientModel:\n",
    "    def __init__(self,T,est_reward,lr,dim,delta_radius,arms):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.t = 0\n",
    "        self.T = T\n",
    "        self.est_agent_reward = est_reward\n",
    "        self.dim = dim\n",
    "        self.model = GradientBandit(arms,dim)\n",
    "        self.reward_function = ModelReward(arms,dim)\n",
    "        self.opt = torch.optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "\n",
    "    def get_policy(self):\n",
    "        return self.model.get_hyperplanes()\n",
    "\n",
    "    def observe_reward(self, context):\n",
    "        self.opt.zero_grad()\n",
    "        context = torch.tensor(context).reshape(1,-1).float()\n",
    "        est_agent_rew = torch.tensor(self.est_agent_reward.get_reward(context)).float()\n",
    "        y_hat = self.model.forward(context,est_agent_rew)\n",
    "        print(y_hat)\n",
    "        reward = self.reward_function.get_reward(y_hat,context)\n",
    "        #loss = self.criterion(reward,torch.zeros(1))\n",
    "        reward.backward()\n",
    "        self.opt.step()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:576: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x) - f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2012, 0.2915, 0.1910, 0.3163]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173: UserWarning: Error detected in MmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\asyncio\\base_events.py\", line 1899, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2880, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2935, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3134, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\DeanA\\AppData\\Local\\Temp\\ipykernel_14648\\4188443012.py\", line 23, in <cell line: 16>\n",
      "    r = model.observe_reward(xp)\n",
      "  File \"C:\\Users\\DeanA\\AppData\\Local\\Temp\\ipykernel_14648\\1026163876.py\", line 20, in observe_reward\n",
      "    y_hat = self.model.forward(context,est_agent_rew)\n",
      "  File \"c:\\Users\\DeanA\\Desktop\\misc\\cs6784_finalproject\\utils.py\", line 87, in forward\n",
      "    x_prime = self.argmax(x, agent_rewards)\n",
      "  File \"c:\\Users\\DeanA\\Desktop\\misc\\cs6784_finalproject\\utils.py\", line 50, in argmax\n",
      "    distances = torch.abs(x_aug @ torch.t(hyperplanes))\n",
      " (Triggered internally at  C:\\b\\abs_f0dma8qm3d\\croot\\pytorch_1669187301762\\work\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:104.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 6]], which is output 0 of SubBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DeanA\\Desktop\\misc\\cs6784_finalproject\\strat_cont_band.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DeanA/Desktop/misc/cs6784_finalproject/strat_cont_band.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m total_strat_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m strat_reward\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DeanA/Desktop/misc/cs6784_finalproject/strat_cont_band.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# x, nostrat_reward = nostrat_agents.generate_context(policy)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DeanA/Desktop/misc/cs6784_finalproject/strat_cont_band.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# total_nostrat_reward += nostrat_reward\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/DeanA/Desktop/misc/cs6784_finalproject/strat_cont_band.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m r \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mobserve_reward(xp)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DeanA/Desktop/misc/cs6784_finalproject/strat_cont_band.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m model_rewards\u001b[39m.\u001b[39mappend(r)\n",
      "\u001b[1;32mc:\\Users\\DeanA\\Desktop\\misc\\cs6784_finalproject\\strat_cont_band.ipynb Cell 11\u001b[0m in \u001b[0;36mStrategyAwareGradientModel.observe_reward\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DeanA/Desktop/misc/cs6784_finalproject/strat_cont_band.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_function\u001b[39m.\u001b[39mget_reward(y_hat,context)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DeanA/Desktop/misc/cs6784_finalproject/strat_cont_band.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#loss = self.criterion(reward,torch.zeros(1))\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/DeanA/Desktop/misc/cs6784_finalproject/strat_cont_band.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m reward\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DeanA/Desktop/misc/cs6784_finalproject/strat_cont_band.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DeanA/Desktop/misc/cs6784_finalproject/strat_cont_band.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m reward\n",
      "File \u001b[1;32mc:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\DeanA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 6]], which is output 0 of SubBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "T = 1\n",
    "ARMS = 4\n",
    "CONTEXT_DIM = 5\n",
    "DELTA = 0.5\n",
    "lr = 0.5\n",
    "\n",
    "private_types = np.random.rand(T,CONTEXT_DIM)\n",
    "agent_rewards = ConstantReward(ARMS)\n",
    "strat_agents = StrategicAgents(private_types,CONTEXT_DIM,DELTA,agent_rewards)\n",
    "nostrat_agents = NonStrategicAgents(private_types,CONTEXT_DIM,DELTA,agent_rewards)\n",
    "model = StrategyAwareGradientModel(T,agent_rewards,lr,CONTEXT_DIM,DELTA,ARMS)\n",
    "total_strat_reward = 0\n",
    "total_nostrat_reward = 0\n",
    "model_rewards = []\n",
    "for i in range(T):\n",
    "    policy = model.get_policy().detach().numpy() #.T[:-1]\n",
    "    print(policy.shape)\n",
    "    xp, strat_reward = strat_agents.generate_context(policy)\n",
    "    total_strat_reward += strat_reward\n",
    "    # x, nostrat_reward = nostrat_agents.generate_context(policy)\n",
    "    # total_nostrat_reward += nostrat_reward\n",
    "    r = model.observe_reward(xp)\n",
    "    model_rewards.append(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12e07b149c03f77d4bac3a4791c0a52eb40c19a4b80558fffc7bbb9c81e36b37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
