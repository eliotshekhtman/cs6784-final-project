{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import dual_annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantReward:\n",
    "    def __init__(self,arms):\n",
    "        reward = np.random.rand(arms)\n",
    "        reward /= np.sum(reward)\n",
    "        self.reward = reward\n",
    "\n",
    "    def get_reward(self,context):\n",
    "        return self.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategicAgents:\n",
    "    def __init__(self, private_types, dim, delta_radius, reward):\n",
    "        self.private_types = private_types\n",
    "        self.delta_radius = delta_radius\n",
    "        self.t = 0\n",
    "        self.reward = reward\n",
    "        self.dim = dim\n",
    "\n",
    "    def generate_context(self, policy):\n",
    "        priv = self.private_types[self.t]\n",
    "        reward = self.reward.get_reward(priv)\n",
    "        bounds = [(-self.delta_radius, self.delta_radius)] * (self.dim)\n",
    "        def objective(delta):\n",
    "            if np.linalg.norm(delta) > self.delta_radius:\n",
    "                return np.inf\n",
    "            x_prime = (priv+delta)\n",
    "            return -reward[np.argmax(x_prime@policy)]\n",
    "\n",
    "        # norm_constraint = NonlinearConstraint(lambda x : np.linalg.norm(x), 0, self.delta_radius)\n",
    "        opt_delt = dual_annealing(objective, bounds)\n",
    "        x_prime = priv + opt_delt.x\n",
    "        self.t += 1\n",
    "        return x_prime, reward[np.argmax(x_prime@policy)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStrategicAgents:\n",
    "    def __init__(self, private_types, dim, delta_radius, reward):\n",
    "        self.private_types = private_types\n",
    "        self.delta_radius = delta_radius\n",
    "        self.t = 0\n",
    "        self.reward = reward\n",
    "        self.dim = dim\n",
    "\n",
    "    def generate_context(self, policy):\n",
    "        priv = self.private_types[self.t]\n",
    "        reward = self.reward.get_reward(priv)\n",
    "\n",
    "        self.t += 1\n",
    "        return priv, reward[np.argmax(priv@policy)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "ARMS = 4\n",
    "CONTEXT_DIM = 5\n",
    "delta_radius = 0.5\n",
    "\n",
    "private_types = np.random.rand(T,CONTEXT_DIM)\n",
    "rewards = ConstantReward(ARMS)\n",
    "strat_agents = StrategicAgents(private_types,CONTEXT_DIM,delta_radius,rewards)\n",
    "nostrat_agents = NonStrategicAgents(private_types,CONTEXT_DIM,delta_radius,rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy: [[ 0.31827152 -0.06689342  0.39410267  0.40178453]\n",
      " [-0.15655195 -0.23157348 -0.12652979 -0.29663049]\n",
      " [ 0.10962736 -0.48593396  0.10674391  0.21765811]\n",
      " [ 0.11098109 -0.25367018 -0.37081219 -0.33286856]\n",
      " [-0.28542119  0.35176982 -0.43220067  0.04631573]] \n",
      "reward: [0.2942194  0.02555277 0.36679221 0.31343562] \n"
     ]
    }
   ],
   "source": [
    "policy = np.random.rand(CONTEXT_DIM,ARMS) - 0.5\n",
    "print(\"policy: {} \".format(policy))\n",
    "print(\"reward: {} \".format(rewards.get_reward(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check to make sure the strategic agents are performing better than non-strategic agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DeanA\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:497: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x) - f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategic reward: 32.27858860198842, non-strategic reward: 29.226927653796203\n"
     ]
    }
   ],
   "source": [
    "total_strat_reward = 0\n",
    "total_nostrat_reward = 0\n",
    "for i in range(T):\n",
    "    xp, strat_reward = strat_agents.generate_context(policy)\n",
    "    x, nostrat_reward = nostrat_agents.generate_context(policy)\n",
    "    total_strat_reward += strat_reward\n",
    "    total_nostrat_reward += nostrat_reward\n",
    "\n",
    "print(\"strategic reward: {}, non-strategic reward: {}\".format(total_strat_reward,total_nostrat_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelReward:\n",
    "    def __init__(self,arms,dim,noise):\n",
    "        self.theta = np.random.rand(arms,dim)\n",
    "\n",
    "    def get_reward(self,action,context):\n",
    "        true_reward = self.theta[action]\n",
    "        return true_reward + random.uniform(-self.noise,self.noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyAwareModel:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5e2642fcaab3346eda04fc03847987821f7f335e1cd6ec4d483a36a9c81821c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
