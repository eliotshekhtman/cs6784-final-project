{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import dual_annealing\n",
    "from utils import GradientBandit, GradientBanditNoStrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantReward:\n",
    "    def __init__(self,arms):\n",
    "        reward = np.random.rand(arms)\n",
    "        reward /= np.sum(reward)\n",
    "        self.reward = reward\n",
    "\n",
    "    def get_reward(self,context):\n",
    "        return self.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategicAgents:\n",
    "    def __init__(self, dim, delta_radius, reward):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.reward = reward\n",
    "        self.dim = dim\n",
    "\n",
    "    def generate_context(self, policy, priv):\n",
    "        reward = self.reward.get_reward(priv)\n",
    "        bounds = [(-self.delta_radius, self.delta_radius)] * (self.dim)\n",
    "        def objective(delta):\n",
    "            if np.linalg.norm(delta) > self.delta_radius:\n",
    "                return np.inf\n",
    "            #x_prime = np.append((priv+delta),[1])\n",
    "            x_prime = priv+delta\n",
    "            return -reward[np.argmax(policy@x_prime)]\n",
    "\n",
    "        opt_delt = dual_annealing(objective, bounds)\n",
    "        burden = np.linalg.norm(opt_delt.x)\n",
    "        x_prime = priv + opt_delt.x\n",
    "\n",
    "        if np.argmax(policy@x_prime) == np.argmax(policy@priv):\n",
    "            x_prime = priv\n",
    "            burden = 0\n",
    "\n",
    "        return x_prime, burden\n",
    "        #return x_prime, reward[np.argmax(policy@np.append(x_prime,[1]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImperfectStrategicAgents:\n",
    "    def __init__(self, dim, delta_radius, reward, policy_noise):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.reward = reward\n",
    "        self.dim = dim\n",
    "        self.policy_noise = policy_noise\n",
    "\n",
    "    def generate_context(self, policy, priv):\n",
    "        imp_policy = policy + self.policy_noise*(np.random.random(policy.shape)-0.5)\n",
    "        reward = self.reward.get_reward(priv)\n",
    "        bounds = [(-self.delta_radius, self.delta_radius)] * (self.dim)\n",
    "        def objective(delta):\n",
    "            if np.linalg.norm(delta) > self.delta_radius:\n",
    "                return np.inf\n",
    "            #x_prime = np.append((priv+delta),[1])\n",
    "            x_prime = priv+delta\n",
    "            return -reward[np.argmax(imp_policy@x_prime)]\n",
    "\n",
    "        opt_delt = dual_annealing(objective, bounds)\n",
    "        burden = np.linalg.norm(opt_delt.x)\n",
    "        x_prime = priv + opt_delt.x\n",
    "\n",
    "        if np.argmax(imp_policy@x_prime) == np.argmax(imp_policy@priv):\n",
    "            x_prime = priv\n",
    "            burden = 0\n",
    "\n",
    "        return x_prime, burden\n",
    "        #return x_prime, reward[np.argmax(policy@np.append(x_prime,[1]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStrategicAgents:\n",
    "    def __init__(self,  dim, delta_radius, reward):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.reward = reward\n",
    "        self.dim = dim\n",
    "\n",
    "    def generate_context(self, policy, priv):\n",
    "        reward = self.reward.get_reward(priv)\n",
    "\n",
    "        return priv, 0\n",
    "        #return priv, reward[np.argmax(policy@np.append(priv,[1]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = 100\n",
    "# ARMS = 4\n",
    "# CONTEXT_DIM = 5\n",
    "# delta_radius = 0.5\n",
    "\n",
    "# private_types = np.random.rand(T,CONTEXT_DIM)\n",
    "# rewards = ConstantReward(ARMS)\n",
    "# strat_agents = StrategicAgents(private_types,CONTEXT_DIM,delta_radius,rewards)\n",
    "# nostrat_agents = NonStrategicAgents(private_types,CONTEXT_DIM,delta_radius,rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = np.random.rand(CONTEXT_DIM,ARMS) - 0.5\n",
    "# print(\"policy: {} \".format(policy))\n",
    "# print(\"reward: {} \".format(rewards.get_reward(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check to make sure the strategic agents are performing better than non-strategic agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_strat_reward = 0\n",
    "# total_nostrat_reward = 0\n",
    "# for i in range(T):\n",
    "#     xp, strat_reward = strat_agents.generate_context(policy)\n",
    "#     x, nostrat_reward = nostrat_agents.generate_context(policy)\n",
    "#     total_strat_reward += strat_reward\n",
    "#     total_nostrat_reward += nostrat_reward\n",
    "\n",
    "# print(\"strategic reward: {}, non-strategic reward: {}\".format(total_strat_reward,total_nostrat_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelReward:\n",
    "    def __init__(self,arms,dim,noise=0.01):\n",
    "        self.theta = np.random.rand(arms,dim)\n",
    "        self.noise_scale = noise\n",
    "\n",
    "    def get_reward(self,action,context):\n",
    "        true_reward = action @ self.theta @ context\n",
    "        return true_reward + (np.random.rand(1)-0.5) * 2*self.noise_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyAwareGradientModel:\n",
    "    def __init__(self,T,est_reward,lr,dim,delta_radius,arms,reward_func):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.t = 0\n",
    "        self.T = T\n",
    "        self.est_agent_reward = est_reward\n",
    "        self.dim = dim\n",
    "        self.model = GradientBandit(arms,dim,bias=False)\n",
    "        self.reward_function = reward_func\n",
    "        self.opt = torch.optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "\n",
    "    def get_policy(self):\n",
    "        return self.model.get_hyperplanes()\n",
    "\n",
    "    def observe_reward(self, context):\n",
    "        self.opt.zero_grad()\n",
    "        context = torch.tensor(context).reshape(1,-1).float()\n",
    "        est_agent_rew = torch.tensor(self.est_agent_reward.get_reward(context)).float()\n",
    "        y_hat = self.model.forward(context,est_agent_rew)\n",
    "        # print(y_hat)\n",
    "        reward = self.reward_function.get_reward(y_hat,context)\n",
    "        loss = self.criterion(1/reward,torch.zeros(1))\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyUnawareGradientModel:\n",
    "    def __init__(self,T,est_reward,lr,dim,delta_radius,arms,reward_func):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.t = 0\n",
    "        self.T = T\n",
    "        self.est_agent_reward = est_reward\n",
    "        self.dim = dim\n",
    "        self.model = GradientBanditNoStrat(arms,dim,bias=False)\n",
    "        self.reward_function = reward_func\n",
    "        self.opt = torch.optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "\n",
    "    def get_policy(self):\n",
    "        return self.model.get_hyperplanes()\n",
    "\n",
    "    def different_(self, x_true, x_observed):\n",
    "        est_agent_rew = self.est_agent_reward.get_reward(None)\n",
    "        y_true = self.model.forward(torch.tensor(x_true).float(),est_agent_rew)\n",
    "        y_obv = self.model.forward(torch.tensor(x_observed).float(),est_agent_rew)\n",
    "        if torch.argmax(y_obv) != torch.argmax(y_true):\n",
    "            better = est_agent_rew[torch.argmax(y_obv)] > est_agent_rew[torch.argmax(y_true)]\n",
    "            return True, better\n",
    "        else:\n",
    "            return False, False\n",
    "\n",
    "    def observe_reward(self, context):\n",
    "        self.opt.zero_grad()\n",
    "        context = torch.tensor(context).reshape(1,-1).float()\n",
    "        est_agent_rew = torch.tensor(self.est_agent_reward.get_reward(context)).float()\n",
    "        y_hat = self.model.forward(context,est_agent_rew)\n",
    "        reward = self.reward_function.get_reward(y_hat,context)\n",
    "        loss = self.criterion(1/reward,torch.zeros(1))\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyUnawareModel:\n",
    "    def __init__(self,T,est_reward,lr,dim,delta_radius,arms,reward_func):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.t = 0\n",
    "        self.T = T\n",
    "        self.est_agent_reward = est_reward\n",
    "        self.dim = dim\n",
    "        self.policy= np.random.rand(arms,dim)\n",
    "        self.reward_function = reward_func\n",
    "        self.committed = False\n",
    "        self.arms = arms\n",
    "        rewards = [ [] for i in range(self.arms)]\n",
    "        contexts = [ [] for i in range(self.arms)]\n",
    "        for arm in range(arms):\n",
    "            rewards[arm] = []\n",
    "            contexts[arm] = []\n",
    "        \n",
    "        self.reward_data = rewards\n",
    "        self.context_data = contexts\n",
    "\n",
    "    def refresh_policy(self):\n",
    "        self.policy= np.random.rand(self.arms,self.dim)        \n",
    "\n",
    "    def get_policy(self):\n",
    "        return self.policy\n",
    "\n",
    "    def committ(self):\n",
    "        theta = np.zeros((self.arms,self.dim))\n",
    "        for arm in range(self.arms):\n",
    "            xs = np.asarray(self.context_data[arm])\n",
    "            ys = np.asarray(self.reward_data[arm])\n",
    "            \n",
    "            est_theta = np.linalg.lstsq(xs,ys,rcond=None)[0]\n",
    "            theta[arm,:] = est_theta.flatten()\n",
    "\n",
    "        self.policy = theta\n",
    "\n",
    "    def different_(self, x_true, x_observed):\n",
    "        est_agent_rew = self.est_agent_reward.get_reward(None)\n",
    "        y_true = self.policy@x_true\n",
    "        y_obv = self.policy@x_observed\n",
    "        if np.argmax(y_obv) != np.argmax(y_true):\n",
    "            better = est_agent_rew[np.argmax(y_obv)] > est_agent_rew[np.argmax(y_true)]\n",
    "            return True, better\n",
    "        else:\n",
    "            return False, False\n",
    "\n",
    "    def get_action(self,context):\n",
    "        pol = self.policy\n",
    "        expected_rewards = self.policy@context\n",
    "        action = np.argmax(expected_rewards)\n",
    "        if self.t > self.T//2:\n",
    "            action_vector = np.zeros(self.arms)\n",
    "            action_vector[action] += 1\n",
    "\n",
    "            new_rewards = (pol@context - self.delta_radius*np.linalg.norm(pol))\n",
    "            if new_rewards[action] < 0:\n",
    "                # TODO: figure out the boundary + new action\n",
    "                pass\n",
    "            \n",
    "\n",
    "        return action\n",
    "\n",
    "    def observe_reward(self, action, context):\n",
    "        est_agent_rew = self.est_agent_reward.get_reward(context)\n",
    "\n",
    "        action_vector = np.zeros(self.arms)\n",
    "        action_vector[action] += 1\n",
    "        reward = self.reward_function.get_reward(action_vector,context)\n",
    "\n",
    "        self.reward_data[action] += [reward]\n",
    "        self.context_data[action] += [context]\n",
    "\n",
    "        return reward, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8, -1. , -1.2])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delt = 0.5\n",
    "pol = np.asarray([[1,-1,0],[0,0,0],[-1,1,0]])\n",
    "action = np.asarray([1,0])\n",
    "context = np.asarray([0.1,-0.1,0])\n",
    "context_org = np.asarray([-.2,.3,0])\n",
    "\n",
    "(pol@context - delt*np.linalg.norm(pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5,  0. ,  0.5])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol@context_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyAwareModel:\n",
    "    def __init__(self,T,est_reward,lr,dim,delta_radius,arms,reward_func):\n",
    "        self.delta_radius = delta_radius\n",
    "        self.t = 0\n",
    "        self.T = T\n",
    "        self.est_agent_reward = est_reward\n",
    "        self.dim = dim\n",
    "        self.policy= np.random.rand(arms,dim)\n",
    "        self.reward_function = reward_func\n",
    "        self.committed = False\n",
    "        self.arms = arms\n",
    "        rewards = [ [] for i in range(self.arms)]\n",
    "        contexts = [ [] for i in range(self.arms)]\n",
    "        for arm in range(arms):\n",
    "            rewards[arm] = []\n",
    "            contexts[arm] = []\n",
    "        \n",
    "        self.reward_data = rewards\n",
    "        self.context_data = contexts\n",
    "\n",
    "    def refresh_policy(self):\n",
    "        self.policy= np.random.rand(self.arms,self.dim)        \n",
    "\n",
    "    def get_policy(self):\n",
    "        return self.policy\n",
    "\n",
    "    def committ(self):\n",
    "        theta = np.zeros((self.arms,self.dim))\n",
    "        for arm in range(self.arms):\n",
    "            xs = np.asarray(self.context_data[arm])\n",
    "            ys = np.asarray(self.reward_data[arm])\n",
    "            \n",
    "            est_theta = np.linalg.lstsq(xs,ys,rcond=None)[0]\n",
    "            theta[arm,:] = est_theta.flatten()\n",
    "\n",
    "        self.policy = theta\n",
    "\n",
    "    def different_(self, x_true, x_observed):\n",
    "        est_agent_rew = self.est_agent_reward.get_reward(None)\n",
    "        y_true = self.policy@x_true\n",
    "        y_obv = self.policy@x_observed\n",
    "        if np.argmax(y_obv) != np.argmax(y_true):\n",
    "            better = est_agent_rew[np.argmax(y_obv)] > est_agent_rew[np.argmax(y_true)]\n",
    "            return True, better\n",
    "        else:\n",
    "            return False, False\n",
    "\n",
    "    def get_action(self,context):\n",
    "        expected_rewards = self.policy@context\n",
    "        action = np.argmax(expected_rewards)\n",
    "        return action\n",
    "\n",
    "    def observe_reward(self, action, context):\n",
    "        est_agent_rew = self.est_agent_reward.get_reward(context)\n",
    "\n",
    "        action_vector = np.zeros(self.arms)\n",
    "        action_vector[action] += 1\n",
    "        reward = self.reward_function.get_reward(action_vector,context)\n",
    "\n",
    "        self.reward_data[action] += [reward]\n",
    "        self.context_data[action] += [context]\n",
    "\n",
    "        return reward, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strat errors: 273 | impf errors: 184\n"
     ]
    }
   ],
   "source": [
    "T = 1200\n",
    "ARMS = 4\n",
    "CONTEXT_DIM = 5\n",
    "DELTA = 1\n",
    "lr = 0.1\n",
    "model_reward_noise = 0.25\n",
    "imperfect_agent_policy_noise = 0.25\n",
    "\n",
    "private_types = np.random.rand(T,CONTEXT_DIM)\n",
    "agent_rewards = ConstantReward(ARMS)\n",
    "model_reward_func = ModelReward(ARMS,CONTEXT_DIM,model_reward_noise)\n",
    "strat_agents = StrategicAgents(CONTEXT_DIM,DELTA,agent_rewards)\n",
    "imperfect_agents = ImperfectStrategicAgents(CONTEXT_DIM,DELTA,agent_rewards,imperfect_agent_policy_noise)\n",
    "nostrat_agents = NonStrategicAgents(CONTEXT_DIM,DELTA,agent_rewards)\n",
    "model = StrategyUnawareModel(T,agent_rewards,lr,CONTEXT_DIM,DELTA,ARMS,model_reward_func)\n",
    "\n",
    "\n",
    "strat_agent_errors = 0\n",
    "impf_agent_error = 0\n",
    "social_buden = 0\n",
    "regret = [0]\n",
    "for i in range(T):\n",
    "    if i % 3 == 0:\n",
    "        x, burden = nostrat_agents.generate_context(model.get_policy(), private_types[i])\n",
    "    elif i % 3 == 1:\n",
    "        x, burden = imperfect_agents.generate_context(model.get_policy(), private_types[i])\n",
    "    else:\n",
    "        x, burden = strat_agents.generate_context(model.get_policy(), private_types[i])\n",
    "    \n",
    "    social_buden += burden\n",
    "    action = model.get_action(x)\n",
    "    reward = model.observe_reward(action,x)\n",
    "    different, better = model.different_(x,private_types[i])\n",
    "    true_act = np.argmax(model_reward_func.theta @ private_types[i])\n",
    "    regret.append(regret[-1] + (true_act != action))\n",
    "    if i < T//2:\n",
    "        model.refresh_policy()\n",
    "    elif i == T//2:\n",
    "        model.committ()\n",
    "\n",
    "    if different:\n",
    "        if i % 3 == 1:\n",
    "            impf_agent_error += 1\n",
    "        else:\n",
    "            strat_agent_errors += 1\n",
    "\n",
    "print(\"strat errors: {} | impf errors: {}\".format(strat_agent_errors,impf_agent_error))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = range(T)\n",
    "plt.scatter(x, regret[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05371876 -0.00421274  0.02893703  0.05027126 -0.07958964]\n",
      " [-0.00438056 -0.01080607  0.04042248 -0.06202191  0.03484464]\n",
      " [ 0.00177615  0.00126685 -0.00351928  0.04317078 -0.00889536]\n",
      " [ 0.02484561 -0.03570046 -0.05702877  0.03369473  0.03814557]]\n"
     ]
    }
   ],
   "source": [
    "print(model.get_policy() - model_reward_func.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "strat: 0.3505486060025757 | impf: 0.33543223262133254 | no strat: 0.3435960223000631\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x = range(T//3)\n",
    "# fig, axs = plt.subplots(3, sharex=True, sharey=False)\n",
    "# print(len(x))\n",
    "# print(len([x.item() for x in model_rewards_strat]))\n",
    "# axs[0].scatter(x, [x.item() for x in model_rewards_strat])\n",
    "# axs[1].scatter(x, [x.item() for x in model_rewards_imp])\n",
    "# axs[2].scatter(x, [x.item() for x in model_rewards_ns])\n",
    "# print(\"strat: {} | impf: {} | no strat: {}\".format(sum([3*x.item() for x in model_rewards_strat])/T, sum([3*x.item() for x in model_rewards_imp])/T,sum([3*x.item() for x in model_rewards_ns])/T,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12e07b149c03f77d4bac3a4791c0a52eb40c19a4b80558fffc7bbb9c81e36b37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
